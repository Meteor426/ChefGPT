'''
æ£€ç´¢ä¼˜åŒ–æ¨¡å—
'''
from typing import List, Dict, Any
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
import logging
import traceback

logger = logging.getLogger(__name__)

class RetrievalOptimizationModule:
    '''
    æ£€ç´¢ä¼˜åŒ–æ¨¡å—,å®ç°æ··åˆæ£€ç´¢å’Œè¿‡æ»¤
    '''
    def __init__(self,vectorstore:FAISS,chunks:List[Document]) :
        '''
        åˆå§‹åŒ–,å¹¶å»ºç«‹ä¸¤ä¸ªæ£€ç´¢å™¨
        Args:
            vectorstore:FAISS ç”¨äºæ‰§è¡Œç¨ å¯†å‘é‡æ£€ç´¢
            chunks:List[Document] æ–‡æœ¬å—ï¼Œç”¨äºæ‰§è¡Œç¨€ç–å‘é‡æ£€ç´¢
        '''
        self.vectorstore = vectorstore
        self.chunks = chunks
        self._set_up_retrievers()
        

    def _set_up_retrievers(self):
        '''
        åˆ›å»ºä¸¤ä¸ªæ£€ç´¢å™¨
        ä¸€ä¸ªå¯†é›†å‘é‡æ£€ç´¢å™¨BM25
        ä¸€ä¸ªç¨€ç–å‘é‡æ£€ç´¢å™¨
        '''
        try:
            self.vector_retriever = self.vectorstore.as_retriever(
                search_type = "similarity",
                k = 5
            )
        except Exception as e:
            print("=== æ•è·å¼‚å¸¸ ===")
            traceback.print_exc()   # <-- è¿™ä¸ªä¼šå¼ºåˆ¶æ‰“å°å®Œæ•´çš„traceback
            print("å¼‚å¸¸ç±»å‹ï¼š", type(e))
            print("å¼‚å¸¸ä¿¡æ¯ï¼š", e)
        try:
            self.bm25_retriever = BM25Retriever.from_documents(
            self.chunks,
            k = 5
            )
        except Exception as e:
            print("=== æ•è·å¼‚å¸¸ ===")
            traceback.print_exc()   # <-- è¿™ä¸ªä¼šå¼ºåˆ¶æ‰“å°å®Œæ•´çš„traceback
            print("å¼‚å¸¸ç±»å‹ï¼š", type(e))
            print("å¼‚å¸¸ä¿¡æ¯ï¼š", e)
        

    def hybrid_search(self,query:str,top_k:int = 3)->List[Document]:
        '''
        æ··åˆæ£€ç´¢,åŸºäºå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢,ä½¿ç”¨RRFé‡æ’
        Args:
            query:str æŸ¥è¯¢æ–‡æœ¬
            top_k:int = 3 è¿”å›æœç´¢æ’åå‰ä¸‰çš„æ£€ç´¢æ–‡æ¡£
        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æœ¬
        '''
        #åˆ†åˆ«è·å–ä¸¤ç§æ£€ç´¢çš„ç»“æœ
        vector_docs = self.vector_retriever.get_relevant_documents(query)
        BM25_docs = self.bm25_retriever.get_relevant_documents(query)
        print("ğŸ” [BM25 æ£€ç´¢ç»“æœ]")
        for i, doc in enumerate(BM25_docs):
            print(f"[{i+1}] èœå: {doc.metadata.get('dish_name')} | chunk_index: {doc.metadata.get('chunk_index')}")

        # æ‰“å°åŸå§‹ å‘é‡ æ£€ç´¢ç»“æœ
        print("ğŸ” [å‘é‡æ£€ç´¢ç»“æœ]")
        for i, doc in enumerate(vector_docs):
            print(f"[{i+1}] èœå: {doc.metadata.get('dish_name')} | chunk_index: {doc.metadata.get('chunk_index')}")
        #ä½¿ç”¨rrfé‡æ’
        reranked_docs = self._rrf_rerank(vector_docs,BM25_docs)
        return reranked_docs[:top_k]

    def _rrf_rerank(self,vector_docs:List[Document],BM25_docs:List[Document]):
        '''
        RRFé‡æ’
        Args:
            vector_docs:å‘é‡æ£€ç´¢çš„ç»“æœ
            BM25_docs:BM25æ£€ç´¢çš„ç»“æœ
        Returns:
            RRFé‡æ’åçš„æ–‡æ¡£
        '''
        rrf_scores = {}
        c = 60 #rrfå‚æ•°
        vec_weight = 1.0
        bm25_weight = 0   
        #è®¡ç®—å‘é‡æ£€ç´¢çš„rrfåˆ†æ•°
        for rank,doc in enumerate(vector_docs):
            #è·å–ä¸€ä¸ªidä½œä¸ºk
            doc_id = hash(doc.page_content)
            score = vec_weight * 1 / (rank + c + 1)
            #rrfè®¡ç®—ï¼Œå› ä¸ºrankæ˜¯ä»0å¼€å§‹ï¼Œæ‰€ä»¥è¦åŠ ä¸€
            rrf_scores[doc_id] = rrf_scores.get(doc_id,0) + score
        
        #è®¡ç®—BM25æ£€ç´¢çš„rrfåˆ†æ•°
        for rank,doc in enumerate(BM25_docs):
            #è·å–ä¸€ä¸ªidä½œä¸ºk
            doc_id = hash(doc.page_content)
            score = bm25_weight * 1 / (rank + c + 1)
            #rrfè®¡ç®—ï¼Œå› ä¸ºrankæ˜¯ä»0å¼€å§‹ï¼Œæ‰€ä»¥è¦åŠ ä¸€
            rrf_scores[doc_id] = rrf_scores.get(doc_id,0) + score

        #åˆå¹¶æ‰€æœ‰æ–‡æ¡£å¹¶æ ¹æ®rrfåˆ†æ•°æ’åº
        #æ ¹æ®idåˆå¹¶ kä¸ºhash(doc.page_content)ï¼Œvä¸ºæ–‡ä»¶æœ¬èº«
        all_docs = {hash(doc.page_content):doc for doc in vector_docs + BM25_docs}
        #æ’åº
        sorted_docs = sorted(all_docs.items(),
                            key=lambda x:rrf_scores.get(x[0],0),   #å› ä¸ºitemsè¿”å›çš„æ˜¯å…ƒç»„çš„é›†åˆï¼Œ{101: "doc1", 102: "doc2"}
                                                                   #æ‰€ä»¥æˆ‘ä»¬ç”¨è¾“å…¥æ–‡æ¡£xçš„x[0]æ¥è·å–é”®
                            reverse=True
                            )
        #åªéœ€è¦æ’å®Œåºåçš„docè¿”å›å³å¯
        for i, (doc_id, doc) in enumerate(sorted_docs[:5]):
            print(f"[RRFæ’åº-{i+1}] èœåï¼š{doc.metadata.get('dish_name', 'æœªçŸ¥')} | RRFåˆ†æ•°ï¼š{rrf_scores[doc_id]:.4f}")
        return [doc for _,doc in sorted_docs]
    
    def metadata_filtered_search(self,query:str,filters:Dict[str,Any],
                                top_k:int = 5 
                                )-> List[Document]:
        '''
        å¸¦æœ‰å…ƒæ•°æ®è¿‡æ»¤çš„æœç´¢
        Args:
            query:æŸ¥è¯¢
            filter:å…ƒæ•°æ®è¿‡æ»¤çš„æ¡ä»¶ ä¾‹å¦‚: filters = {"category": "èœè°±", "difficulty": ["ç®€å•", "ä¸­ç­‰"]}
            top_k:è¿”å›çš„æ–‡æ¡£æ•°é‡
        Returns:
            è¿”å›æœç´¢ç»“æœ
        '''
        #å…ˆåˆ©ç”¨æ··åˆæ£€ç´¢å¾—åˆ°åˆæ­¥ç»“æœ,è¿™ä¸€æ­¥æŠŠtop_kè°ƒå¤§ï¼Œè·å–æ›´å¤šå€™é€‰ç»“æœç”¨äºè¿‡æ»¤
        original_docs = self.hybrid_search(query,top_k*3)

        #å†è¿›è¡Œå…ƒæ•°æ®è¿‡æ»¤ è¿™ä¸ªç”¨æ³•å¯ä»¥å›ºå®šè®°ä½
        filter_docs = []
        for doc in original_docs:    #allå‡½æ•° åˆ¤æ–­åˆ—è¡¨é‡Œæ¯ä¸€ä¸ªéƒ½ä¸ºTrueæ‰è¿”å›Trueï¼Œç›¸å¯¹çš„ï¼Œanyå‡½æ•°åªè¦æœ‰ä¸€ä¸ªå°±è¿”å›True
            match = all(
                (
                    doc.metadata.get(k) in v if isinstance(v,list)  #å¦‚æœæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ¤æ–­æ˜¯å¦åœ¨åˆ—è¡¨å†…
                    else doc.metadata.get(k) == v   #å¦‚æœæ˜¯ä¸€ä¸ªå€¼ï¼Œåˆ¤æ–­æ˜¯å¦ç›¸ç­‰
                )
                for k,v in filter.items()   #éå†æ¯ä¸€ä¸ªè¿‡æ»¤æ¡ä»¶
            )
            if match:
                filter_docs.append(doc)
                if (len(filter_docs) >= top_k):
                    break
        return filter_docs






        